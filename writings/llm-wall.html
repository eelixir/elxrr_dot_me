<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>elixir - moving beyond the llm wall</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header class="header">
        <nav class="nav">
            <div class="nav-container">
                <a href="../index.html" class="nav-link">home</a>
                <a href="../writings.html" class="nav-link">writings</a>
                <a href="../bookmarks.html" class="nav-link">bookmarks</a>
            </div>
        </nav>
        <div class="header-separator"></div>
    </header>

    <main class="blog-main">
        <div class="blog-post-header">
            <h1 class="blog-post-title">Moving Beyond the LLM Wall</h1>
            <div class="blog-post-meta">
                <span class="blog-post-date">12-08-2025</span>
                <span class="read-time">5 min read</span>
            </div>
        </div>

        <div class="header-separator"></div>


        <article class="blog-post-content">
            <p></p>
            <p>LLMs over the past three years, since the initial release of ChatGPT in November 2022, have been rapidly improving. However, with the recent launch of GPT-5, it has become apparent that LLMs in their current nature are reaching a plateau.</p>
            <p>The future of AI isn't purely going to be in creating more powerful models, but rather the seamless, practical integration of AI into the software and hardware we are already using every day. Making AI frictionless and existing outside of the chatbox interface is the next natural step in the advancement of AI.</p>
            <p>First of all, we should understand exactly what an LLM is. A large language model (LLM) is an AI system trained to understand and generate human-like text. It's built using a neural network architecture with billions of parameters, mathematical weights that capture patterns in language. During training, massive amounts of data are inputted to the model and it learns to predict the next word in a sequence. This prediction ability then allows it to answer questions, write essays, and generate code.</p>


            <h3>The LLM Wall</h3>
            <p>So why are LLMs hitting a wall? A core reason is due to data saturation. To produce better LLMs, you have to input more data. But there's only so much data you can provide. The models are beginning to run out of high-quality, publicly available data to train on. </p>
            <p>Another reason is due to the fundamental limitations of an LLM. They are excellent at statistical approximation, but they lack genuine reasoning. To put it simply, they lack the common sense of a human.</p>
            <p>Hallucinations are another common issue that is faced when dealing with LLMs. They will confidently provide factually incorrect information as factual. The reason why hallucinations occur is also due to the fundamental way that LLMs function. They are optimised to predict the next plausible word in a sequence, not for producing factually accurate information.</p>
            <p>In summary, AGI is highly unlikely to be achieved by upscaling current LLM architecture due to the way they fundamentally work and the lack of true reasoning.</p>

            <h3>Integrating AI into Existing Products and Software</h3>
            <p>The current future of AI is not in creating a new model with marginally improved benchmarks, but rather in integrating current models into existing software and hardware.</p>
            <p>AI should move away from standalone chat interfaces and into tools we are already using in our everyday lives. Take, for example, the implementation of Grok on X. It is directly implemented into the social media platform on every post. Don't understand something in a post you're reading? Click a singular button and have Grok summarise and explain it within seconds.</p>
            <p>Cluely is another product that implements AI outside of a chatbox and directly onto your desktop. Cluely offers a real-time AI assistant that responds with context of what's on your screen or what's happening in your conversation. The product works by analysing a user's microphone and screen, then delivering instant, context-aware suggestions, answers, and data. </p>
            <p>These are just some examples of AI that have already been implemented into software. The limitations of what can be done are yet to be explored. </p>

            <h3>The Rise of AI Hardware</h3>     
            <p>Hardware is another way AI can be implemented into our everyday lives. Over the next few years, you will see many companies creating AI integrated hardware. AI will begin to leave our screens and be implemented into hardware devices, wearables, robots, drones, etc. </p>      
            <p>Friend.com is an upcoming device that plans to bring AI to the physical realm. It is a wearable companion designed to provide a constant, subtle companionship. It works by listening to its environment and communicating via text messages, creating a personalised presence.</p>
            <p>Other examples of similar devices, although slightly gimmicky, are Rabbit R1 and Humane's AI Pin. These devices are dedicated hardware products built specifically to interact with an AI model. They are a physical manifestation of a conversational AI, designed for hands-free, real-time interaction.</p>
            <p>As we see the emergence of robots into households, such as Tesla bots, they will utilise AI technology. They will have real-time, reinforcement learning where they can dynamically adapt and problem solve. The robot will be able to continuously update its skills and knowledge without interruption. For example, with real-time learning, a human could be able to teach the robot how to use a washing machine without any previous data training on that topic. This is not possible when a robot is pre-programmed or trained on a static dataset.</p>
            <p>The use of AI in hardware will signify a shift in the way AI is utilised in our lives.</p>

            <h3>The Benefits of This New Direction</h3>
            <p>Why is this the direction AI should take? To put it simply, it is the path it has to and is going to take. Making AI seamless and convenient is crucial for enhancing its capabilities. Having it built-in will remove the friction that currently exists with the chatbot interface. </p>
            <p>We should also focus on the privacy aspect of AI. With the recent headlines of ChatGPT shared chats being publicly indexed in Google searches, it is more crucial than ever to ensure transparency regarding the privacy of AI and how your data is handled. Think about what information you're sending to AI and consider self-hosting in certain use cases. </p>
            <p>Personalisation is also another important factor when thinking about the future of AI. Currently, all LLMs are very generic; in the future, they will become more personalized to individuals and the specific use case of that individual to maximize efficiency.</p>
            <p>In conclusion, I believe that the LLM race is slowing down, and real innovation and the future of AI technology lie in integrating AI into existing and new products. The next wave of AI technology will be defined by its specialization, not its generality; or maybe DeepSeek will release AGI next week.</p>

        </article>
    </main>

    <footer class="universal-footer">
        <div class="footer-separator"></div>
        <div class="footer-links">
            <a href="https://x.com/elixrxyz" target="_blank">x/twtr</a>
            <a href="https://github.com/eelixir" target="_blank">github</a>
            <a href="https://signal.me/#eu/tdSbTyowZQKt9s3ppgnJfyWoAiro51lbDwAOSuyQhRVk2FfFiyRL5YcMx12rIs0V" target="_blank">signal</a>
        </div>
    </footer>
</body>
</html>